{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMXZB5s0UXMxzx0femqD6Fc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import yaml\n","\n","import cv2\n","import torch\n","from torch.autograd import Variable\n","\n","from models.yolov3 import *\n","from utils.utils import *\n","from utils.parse_yolo_weights import parse_yolo_weights\n","\n","from captum.attr import GradientShap\n","\n","import os\n","import glob\n","\n","from utils.vis_bbox import vis_bbox\n","import matplotlib.pyplot as plt\n","\n","import json"],"metadata":{"id":"fmuZNzVD-U2-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualization\n","You need to input the image, the information about your model, and the output index."],"metadata":{"id":"Yk1vsi8Bz7Sl"}},{"cell_type":"code","source":["cfg = '' #config file path\n","gpu = 0\n","image = '' #image file path\n","baselines = None\n","target_y == 'cls'\n","output_id = [0,0,0,0] #output index (layer anchor, x, y)\n","weights_path = ''\n","ckpt = ''\n","\n","with open(cfg, 'r') as f:\n","    cfg = yaml.load(f)\n","imgsize = cfg['TEST']['IMGSIZE']\n","model = YOLOv3(cfg['MODEL'])\n","num_classes = cfg['MODEL']['N_CLASSES']\n","\n","confthre = cfg['TEST']['CONFTHRE']\n","nmsthre = cfg['TEST']['NMSTHRE']\n","if gpu >= 0:\n","    model.cuda(gpu)    \n","\n","\n","assert weights_path or ckpt, 'One of --weights_path and --ckpt must be specified'\n","\n","if weights_path:\n","    print(\"loading yolo weights %s\" % (weights_path))\n","    parse_yolo_weights(model, weights_path)\n","elif ckpt:\n","    print(\"loading checkpoint %s\" % (ckpt))\n","    state = torch.load(ckpt)\n","    if 'model_state_dict' in state.keys():\n","        model.load_state_dict(state['model_state_dict'])\n","    else:\n","        model.load_state_dict(state)\n","\n","model.eval()\n","\n","img = cv2.imread(image_path)\n","img_raw = img.copy()[:, :, ::-1].transpose((2, 0, 1))\n","img, info_img = preprocess(img, imgsize, jitter=0)  # info = (h, w, nh, nw, dx, dy)\n","img = np.transpose(img / 255., (2, 0, 1))\n","img = torch.from_numpy(img).float().unsqueeze(0)\n","\n","if gpu >= 0:\n","    img = Variable(img.type(torch.cuda.FloatTensor))\n","else:\n","    img = Variable(img.type(torch.FloatTensor))\n","\n","if baselines==None:\n","    baselines = img * 0    \n","    \n","if gpu >= 0:\n","    img = Variable(img.type(torch.cuda.FloatTensor))\n","    baselines = Variable(baselines.type(torch.cuda.FloatTensor))\n","else:\n","    img = Variable(img.type(torch.FloatTensor))\n","    baselines = Variable(baselines.type(torch.FloatTensor))\n","\n","if target_y == 'obj':\n","    target_y = 4\n","elif target_y == 'cls':\n","    target_y = 5\n","\n","\n","# setting wrapper\n","def yolo_wrapper(inp, output_id):\n","    layer_num, anchor_num, x, y = output_id\n","    output = model(inp, shap=True)\n","    return output[layer_num][:,anchor_num,y,x]\n","                \n","with torch.no_grad():\n","    gs = GradientShap(yolo_wrapper,multiply_by_inputs=multiply_by_inputs)\n","    \n","with torch.no_grad():\n","    attr, delta = gs.attribute(img, additional_forward_args=output_id, n_samples=n_samples, stdevs=stdevs, baselines=baselines, target=target_y, return_convergence_delta=True)\n","# postprocessing of attribution\n","attr = np.transpose(attr.squeeze().cpu().detach().numpy(), (1,2,0))\n","original_image = np.transpose(img.squeeze().cpu().detach().numpy(), (1,2,0))\n","# visualization of attribution\n","pos_fig, pos_axis = viz.visualize_image_attr(attr,\n","                                                original_image,\n","                                                \"heat_map\",\n","                                                \"positive\",\n","                                                cmap=\"Reds\",\n","                                                show_colorbar=True,\n","                                                fig_size=(8,6))\n","neg_fig, neg_axis = viz.visualize_image_attr(attr,\n","                                                original_image,\n","                                                \"heat_map\",\n","                                                \"positive\",\n","                                                cmap=\"Blues\",\n","                                                show_colorbar=True,\n","                                                fig_size=(8,6))\n"],"metadata":{"id":"wcXejiMB0BS5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation/Data Selection with SHAP"],"metadata":{"id":"Wyti2QWG4Tdv"}},{"cell_type":"code","source":["cfg = '' #config file path\n","gpu = 0\n","image = '' #image file path\n","baselines = None\n","target_y == 'cls'\n","output_id = [0,0,0,0] #output index (layer anchor, x, y)\n","weights_path = ''\n","ckpt = ''\n","bboxes = [] #bounding box index list\n","\n","with open(cfg, 'r') as f:\n","    cfg = yaml.load(f)\n","imgsize = cfg['TEST']['IMGSIZE']\n","model = YOLOv3(cfg['MODEL'])\n","num_classes = cfg['MODEL']['N_CLASSES']\n","\n","confthre = cfg['TEST']['CONFTHRE']\n","nmsthre = cfg['TEST']['NMSTHRE']\n","if gpu >= 0:\n","    model.cuda(gpu)    \n","\n","\n","assert weights_path or ckpt, 'One of --weights_path and --ckpt must be specified'\n","\n","if weights_path:\n","    print(\"loading yolo weights %s\" % (weights_path))\n","    parse_yolo_weights(model, weights_path)\n","elif ckpt:\n","    print(\"loading checkpoint %s\" % (ckpt))\n","    state = torch.load(ckpt)\n","    if 'model_state_dict' in state.keys():\n","        model.load_state_dict(state['model_state_dict'])\n","    else:\n","        model.load_state_dict(state)\n","\n","model.eval()\n","\n","img = cv2.imread(image_path)\n","img_raw = img.copy()[:, :, ::-1].transpose((2, 0, 1))\n","img, info_img = preprocess(img, imgsize, jitter=0)  # info = (h, w, nh, nw, dx, dy)\n","img = np.transpose(img / 255., (2, 0, 1))\n","img = torch.from_numpy(img).float().unsqueeze(0)\n","\n","if gpu >= 0:\n","    img = Variable(img.type(torch.cuda.FloatTensor))\n","else:\n","    img = Variable(img.type(torch.FloatTensor))\n","\n","if baselines==None:\n","    baselines = img * 0    \n","    \n","if gpu >= 0:\n","    img = Variable(img.type(torch.cuda.FloatTensor))\n","    baselines = Variable(baselines.type(torch.cuda.FloatTensor))\n","else:\n","    img = Variable(img.type(torch.FloatTensor))\n","    baselines = Variable(baselines.type(torch.FloatTensor))\n","\n","if target_y == 'obj':\n","    target_y = 4\n","elif target_y == 'cls':\n","    target_y = 5\n","\n","\n","# setting wrapper\n","def yolo_wrapper(inp, output_id):\n","    layer_num, anchor_num, x, y = output_id\n","    output = model(inp, shap=True)\n","    return output[layer_num][:,anchor_num,y,x]\n","                \n","with torch.no_grad():\n","    gs = GradientShap(yolo_wrapper,multiply_by_inputs=multiply_by_inputs)\n","    \n","with torch.no_grad():\n","    attr, delta = gs.attribute(img, additional_forward_args=output_id, n_samples=n_samples, stdevs=stdevs, baselines=baselines, target=target_y, return_convergence_delta=True)\n","\n","img_H = 416\n","img_W = 416\n","\n","def zscore(x, axis = None):\n","    xmean = x.mean(axis=axis, keepdims=True)\n","    xstd  = np.std(x, axis=axis, keepdims=True)\n","    zscore = (x-xmean)/xstd\n","    return zscore\n","\n","pixel_attr = np.sum(attr, axis=2)\n","pixel_attr = zscore(pixel_attr)\n","pixel_mask = np.zeros((img_H,img_W))\n","in_area = 0\n","for [x1,y1,x2,y2] in bboxes:\n","    for i in range(int(y1), int(y2)):\n","        for j in range(int(x1),int(x2)):\n","            pixel_mask[i,j] = 1\n","            in_area += 1\n","out_area = img_H*img_W - in_area\n","\n","in_pos = 0\n","in_neg = 0\n","out_pos = 0\n","out_neg = 0\n","l_in = []\n","for i in range(img_H):\n","    for j in range(img_W):\n","        if pixel_mask[i,j]>0:\n","            if pixel_attr[i,j]>=0:\n","                in_pos += pixel_attr[i,j]\n","                l_in.append(pixel_attr[i,j])\n","            else:\n","                in_neg += pixel_attr[i,j]\n","            \n","        else:\n","            if pixel_attr[i,j]>=0:\n","                out_pos += pixel_attr[i,j]\n","            else:\n","                out_neg += pixel_attr[i,j]\n","                \n","if in_area >0:\n","    in_pos = in_pos/in_area\n","    in_neg = in_neg/in_area\n","if out_area>0:\n","    out_pos = out_pos/out_area\n","    out_neg = out_neg/out_area\n"],"metadata":{"id":"uQiX1hTj4SYU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training with SHAP-Regularization\n","We used annotation json files which contain dict format data like\n","* {\"image_file_name\":\n","    * {\"regions\":\n","        * [ {\"class_id\": 0, \"bb\":[0,0,0,0]},..."],"metadata":{"id":"u8Sblu4L4icq"}},{"cell_type":"code","source":["!python train_vd_reg.py --cfg config/config.cfg --weights_path weights/darknet53.conv.74 --checkpoint_interval 100 --checkpoint_dir checkpoints --anno_file anno_data.json --shap_interval 10 --eval_interval 10"],"metadata":{"id":"QE-XT6a-4kkb"},"execution_count":null,"outputs":[]}]}